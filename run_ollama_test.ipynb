{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.ollama import Ollama\n",
    "\n",
    "# Initialize the first model\n",
    "llm1 = Ollama(model=\"qwen2.5:72b\", request_timeout=120.0)\n",
    "\n",
    "# Initialize the second model\n",
    "llm2 = Ollama(model=\"deepseek-r1:70b\", request_timeout=120.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_models(prompt):\n",
    "    # Query the first model\n",
    "    response1 = llm1.complete(prompt)\n",
    "\n",
    "    # Query the second model\n",
    "    response2 = llm2.complete(prompt)\n",
    "\n",
    "    return response1, response2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Translate the following English text to French: 'Hello, how are you?'\"\n",
    "response1, response2 = query_models(prompt)\n",
    "\n",
    "print(\"Response from Model 1:\", response1)\n",
    "print(\"Response from Model 2:\", response2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.ollama import Ollama\n",
    "\n",
    "# Initialize the first model\n",
    "llm1 = Ollama(model=\"qwen2.5:72b\", request_timeout=120.0)\n",
    "llm2 = Ollama(model=\"llama3.3:latest\", request_timeout=120.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.tools import BaseTool, FunctionTool\n",
    "_memory = \"user name is abc\"\n",
    "\n",
    "def current_memory_function() -> str:\n",
    "    \"\"\"Returns the current global memory string.\n",
    "    \n",
    "    Returns:\n",
    "        str: The accumulated memory string (editable globally), \n",
    "             initially empty\n",
    "    \"\"\"\n",
    "    return _memory\n",
    "current_memory_function_tool = FunctionTool.from_defaults(fn=current_memory_function)\n",
    "\n",
    "def update_memory(new_info: str) -> None:\n",
    "    \"\"\"Updates the global memory with new information if not already present.\n",
    "    \n",
    "    Args:\n",
    "        new_info (str): The string to add to memory if it contains \n",
    "                        previously unknown information\n",
    "    \"\"\"\n",
    "    global _memory\n",
    "    entries = _memory.split('\\n') if _memory else []\n",
    "    \n",
    "    # Add only if new information not in existing memory\n",
    "    if new_info.strip() and new_info not in entries:\n",
    "        entries.append(new_info)\n",
    "        _memory = '\\n'.join(entries)\n",
    "update_memory_tool = FunctionTool.from_defaults(fn=update_memory)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.agent import ReActAgent\n",
    "agent = ReActAgent.from_tools([current_memory_function_tool, update_memory_tool], llm=llm1, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = agent.chat(\"What do u know so far\")\n",
    "print(response)\n",
    "response = agent.chat(\"i want to teach you something , i love to be happy\")\n",
    "print(response)\n",
    "response = agent.chat(\"i love sushi\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = agent.chat(\"What do u know so far\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sql engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SQLDatabase\n",
    "from sqlalchemy import create_engine\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "# Configuration for the database\n",
    "@dataclass\n",
    "class Config:\n",
    "    DB_HOST: str = \"143.198.230.83\"\n",
    "    DB_PORT: int = 3306\n",
    "    DB_NAME: str = \"golgixportal\"\n",
    "    DB_USER: str = \"golgix\"\n",
    "    DB_PASSWORD: str = \"preciseV5\"\n",
    "    ALLOWED_TABLES: list[str] = field(default_factory=lambda: [\n",
    "        \"DE\",\n",
    "        \"HPLC_Data\",\n",
    "        \"fermentation_data\",\n",
    "        \"plc_data_1\",\n",
    "        \"plc_data_2\",\n",
    "        \"plc_data_3\",\n",
    "        \"plc_data_4\"\n",
    "    ])\n",
    "\n",
    "# Load configuration\n",
    "config = Config()\n",
    "\n",
    "# Setup the SQLAlchemy engine URI\n",
    "db_uri = f\"mysql+pymysql://{config.DB_USER}:{config.DB_PASSWORD}@{config.DB_HOST}:{config.DB_PORT}/{config.DB_NAME}\"\n",
    "\n",
    "# Initialize the SQLDatabase with the URI and include_tables parameter\n",
    "sql_database = SQLDatabase.from_uri(db_uri, include_tables=config.ALLOWED_TABLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.ollama import Ollama\n",
    "\n",
    "# Initialize the first model\n",
    "llm1 = Ollama(model=\"qwen2.5:72b\", request_timeout=120.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.query_pipeline import QueryPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sagar/anaconda3/envs/factfinder/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🌍 To view the Phoenix app in your browser, visit http://localhost:6006/\n",
      "📖 For more information on how to use Phoenix, check out https://docs.arize.com/phoenix\n"
     ]
    }
   ],
   "source": [
    "# setup Arize Phoenix for logging/observability\n",
    "import phoenix as px\n",
    "import llama_index.core\n",
    "\n",
    "px.launch_app()\n",
    "llama_index.core.set_global_handler(\"arize_phoenix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.query_engine import NLSQLTableQueryEngine\n",
    "from llama_index.core.tools import QueryEngineTool\n",
    "\n",
    "sql_query_engine = NLSQLTableQueryEngine(\n",
    "    sql_database=sql_database,\n",
    "    llm=llm1,\n",
    "    embed_model='local',\n",
    "    # tables=[\"albums\", \"tracks\", \"artists\"],\n",
    "    verbose=True,\n",
    "\n",
    ")\n",
    "sql_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=sql_query_engine,\n",
    "    name=\"sql_tool\",\n",
    "    description=(\n",
    "        \"Useful for translating a natural language query into a SQL query\"\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.query_pipeline import QueryPipeline as QP\n",
    "\n",
    "qp = QP(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.agent.react.types import (\n",
    "    ActionReasoningStep,\n",
    "    ObservationReasoningStep,\n",
    "    ResponseReasoningStep,\n",
    ")\n",
    "from llama_index.core.agent import Task, AgentChatResponse\n",
    "from llama_index.core.query_pipeline import (\n",
    "    StatefulFnComponent,\n",
    "    QueryComponent,\n",
    "    ToolRunnerComponent,\n",
    ")\n",
    "from llama_index.core.llms import MessageRole\n",
    "from typing import Dict, Any, Optional, Tuple, List, cast\n",
    "\n",
    "\n",
    "# Input Component\n",
    "## This is the component that produces agent inputs to the rest of the components\n",
    "## Can also put initialization logic here.\n",
    "def agent_input_fn(state: Dict[str, Any]) -> str:\n",
    "    \"\"\"Agent input function.\n",
    "\n",
    "    Returns:\n",
    "        A Dictionary of output keys and values. If you are specifying\n",
    "        src_key when defining links between this component and other\n",
    "        components, make sure the src_key matches the specified output_key.\n",
    "\n",
    "    \"\"\"\n",
    "    task = state[\"task\"]\n",
    "    if len(state[\"current_reasoning\"]) == 0:\n",
    "        reasoning_step = ObservationReasoningStep(observation=task.input)\n",
    "        state[\"current_reasoning\"].append(reasoning_step)\n",
    "    return task.input\n",
    "\n",
    "\n",
    "agent_input_component = StatefulFnComponent(fn=agent_input_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.agent import ReActChatFormatter\n",
    "from llama_index.core.query_pipeline import InputComponent, Link\n",
    "from llama_index.core.llms import ChatMessage\n",
    "from llama_index.core.tools import BaseTool\n",
    "\n",
    "\n",
    "## define prompt function\n",
    "def react_prompt_fn(\n",
    "    state: Dict[str, Any], input: str, tools: List[BaseTool]\n",
    ") -> List[ChatMessage]:\n",
    "    task = state[\"task\"]\n",
    "    # Add input to reasoning\n",
    "    chat_formatter = ReActChatFormatter()\n",
    "    cur_prompt = chat_formatter.format(\n",
    "        tools,\n",
    "        chat_history=task.memory.get(),\n",
    "        current_reasoning=state[\"current_reasoning\"],\n",
    "    )\n",
    "    return cur_prompt\n",
    "\n",
    "\n",
    "react_prompt_component = StatefulFnComponent(\n",
    "    fn=react_prompt_fn, partial_dict={\"tools\": [sql_tool]}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Set, Optional\n",
    "from llama_index.core.agent.react.output_parser import ReActOutputParser\n",
    "from llama_index.core.llms import ChatResponse\n",
    "from llama_index.core.agent.types import Task\n",
    "\n",
    "\n",
    "def parse_react_output_fn(state: Dict[str, Any], chat_response: ChatResponse):\n",
    "    \"\"\"Parse ReAct output into a reasoning step.\"\"\"\n",
    "    output_parser = ReActOutputParser()\n",
    "    reasoning_step = output_parser.parse(chat_response.message.content)\n",
    "    return {\"done\": reasoning_step.is_done, \"reasoning_step\": reasoning_step}\n",
    "\n",
    "\n",
    "parse_react_output = StatefulFnComponent(fn=parse_react_output_fn)\n",
    "\n",
    "\n",
    "def run_tool_fn(state: Dict[str, Any], reasoning_step: ActionReasoningStep):\n",
    "    \"\"\"Run tool and process tool output.\"\"\"\n",
    "    task = state[\"task\"]\n",
    "    tool_runner_component = ToolRunnerComponent(\n",
    "        [sql_tool], callback_manager=task.callback_manager\n",
    "    )\n",
    "    tool_output = tool_runner_component.run_component(\n",
    "        tool_name=reasoning_step.action,\n",
    "        tool_input=reasoning_step.action_input,\n",
    "    )\n",
    "    observation_step = ObservationReasoningStep(observation=str(tool_output))\n",
    "    state[\"current_reasoning\"].append(observation_step)\n",
    "    # TODO: get output\n",
    "\n",
    "    # return tuple of current output and False for is_done\n",
    "    return observation_step.get_content(), False\n",
    "\n",
    "\n",
    "run_tool = StatefulFnComponent(fn=run_tool_fn)\n",
    "\n",
    "\n",
    "def process_response_fn(\n",
    "    state: Dict[str, Any], response_step: ResponseReasoningStep\n",
    "):\n",
    "    \"\"\"Process response.\"\"\"\n",
    "    state[\"current_reasoning\"].append(response_step)\n",
    "    return response_step.response, True\n",
    "\n",
    "\n",
    "process_response = StatefulFnComponent(fn=process_response_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.query_pipeline import QueryPipeline as QP\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "qp.add_modules(\n",
    "    {\n",
    "        \"agent_input\": agent_input_component,\n",
    "        \"react_prompt\": react_prompt_component,\n",
    "        \"llm\": llm1,\n",
    "        \"react_output_parser\": parse_react_output,\n",
    "        \"run_tool\": run_tool,\n",
    "        \"process_response\": process_response,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# link input to react prompt to parsed out response (either tool action/input or observation)\n",
    "qp.add_chain([\"agent_input\", \"react_prompt\", \"llm\", \"react_output_parser\"])\n",
    "\n",
    "# add conditional link from react output to tool call (if not done)\n",
    "qp.add_link(\n",
    "    \"react_output_parser\",\n",
    "    \"run_tool\",\n",
    "    condition_fn=lambda x: not x[\"done\"],\n",
    "    input_fn=lambda x: x[\"reasoning_step\"],\n",
    ")\n",
    "# add conditional link from react output to final response processing (if done)\n",
    "qp.add_link(\n",
    "    \"react_output_parser\",\n",
    "    \"process_response\",\n",
    "    condition_fn=lambda x: x[\"done\"],\n",
    "    input_fn=lambda x: x[\"reasoning_step\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent_dag.html\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"600px\"\n",
       "            src=\"agent_dag.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x79cd634ee070>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyvis.network import Network\n",
    "\n",
    "net = Network(notebook=True, cdn_resources=\"in_line\", directed=True)\n",
    "net.from_nx(qp.clean_dag)\n",
    "net.show(\"agent_dag.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.agent import FnAgentWorker\n",
    "from typing import Dict, Tuple, Any\n",
    "\n",
    "\n",
    "def run_agent_fn(state: Dict[str, Any]) -> Tuple[Dict[str, Any], bool]:\n",
    "    \"\"\"Run agent function.\"\"\"\n",
    "    task, qp = state[\"__task__\"], state[\"query_pipeline\"]\n",
    "    # if first run, then set query pipeline state to initial variables\n",
    "    if state[\"is_first\"]:\n",
    "        qp.set_state(\n",
    "            {\n",
    "                \"task\": task,\n",
    "                \"current_reasoning\": [],\n",
    "            }\n",
    "        )\n",
    "        state[\"is_first\"] = False\n",
    "\n",
    "    # no explicit input here, just run root node\n",
    "    response_str, is_done = qp.run()\n",
    "    # if done, store output and log to memory\n",
    "    # a core memory module is available in the `task` variable. Of course you can log\n",
    "    # and store your own memory as well\n",
    "    state[\"__output__\"] = response_str\n",
    "    if is_done:\n",
    "        task.memory.put_messages(\n",
    "            [\n",
    "                ChatMessage(content=task.input, role=MessageRole.USER),\n",
    "                ChatMessage(content=response_str, role=MessageRole.ASSISTANT),\n",
    "            ]\n",
    "        )\n",
    "    return state, is_done\n",
    "\n",
    "\n",
    "agent = FnAgentWorker(\n",
    "    fn=run_agent_fn,\n",
    "    initial_state={\"query_pipeline\": qp, \"is_first\": True},\n",
    ").as_agent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start task\n",
    "task = agent.create_task(\n",
    "    \"Downtime on equipment last 24 hours\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;2;155;135;227m> Running module agent_input with input: \n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module react_prompt with input: \n",
      "input: Downtime on equipment last 24 hours\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module llm with input: \n",
      "messages: [ChatMessage(role=<MessageRole.SYSTEM: 'system'>, additional_kwargs={}, blocks=[TextBlock(block_type='text', text='You are designed to help with a variety of tasks, from answering questions to providi...\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module react_output_parser with input: \n",
      "chat_response: assistant: Thought: I can answer without using any more tools. I'll use the user's language to answer.\n",
      "Answer: In the last 24 hours, there has been no downtime on the equipment. The query did not retu...\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module process_response with input: \n",
      "response_step: thought=\"I can answer without using any more tools. I'll use the user's language to answer.\" response='In the last 24 hours, there has been no downtime on the equipment. The query did not return any r...\n",
      "\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "step_output = agent.run_step(task.task_id)\n",
    "step_output = agent.run_step(task.task_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step_output.is_last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the last 24 hours, there has been no downtime on the equipment. The query did not return any records where the flow rate (FI_3150_PV) was zero, indicating that the equipment has been operational throughout this period.\n"
     ]
    }
   ],
   "source": [
    "response = agent.finalize_response(task.task_id)\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "factfinder",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
